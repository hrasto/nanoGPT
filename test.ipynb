{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "import segutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.memmap('data/shakespeare_char/train.bin', dtype=np.uint16, mode='r')\n",
    "data_val = np.memmap('data/shakespeare_char/val.bin', dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111540, 1003854)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_val), len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'shakespeare_char'\n",
    "block_size = 256\n",
    "batch_size = 64\n",
    "device = 'mps'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data', dataset)\n",
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 012345678  --->  012, 234, 456, 678\n",
    "# 01234567  --->  012, 234, 456, 67  -> last block is not full\n",
    "block_size=3\n",
    "data_size=8\n",
    "n_blocks=4\n",
    "\n",
    "(data_size-1)/(block_size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18 47 56 ... 43 43 42]\n",
      " [42  1 39 ...  0  0 13]\n",
      " [13 50 50 ... 47 64 43]\n",
      " ...\n",
      " [57 43 12 ... 63  0 47]\n",
      " [47 57  1 ...  1 46 43]\n",
      " [43 56 43 ... 51 43  6]]\n",
      "[[18 47 56 ... 43 43 42]\n",
      " [42  1 39 ...  0  0 13]\n",
      " [13 50 50 ... 47 64 43]\n",
      " ...\n",
      " [57 43 12 ... 63  0 47]\n",
      " [47 57  1 ...  1 46 43]\n",
      " [43 56 43 ... 51 43  6]]\n",
      "[[18 47 56 ... 43 43 42]\n",
      " [42  1 39 ...  0  0 13]\n",
      " [13 50 50 ... 47 64 43]\n",
      " ...\n",
      " [57 43 12 ... 63  0 47]\n",
      " [47 57  1 ...  1 46 43]\n",
      " [43 56 43 ... 51 43  6]]\n",
      "[[18 47 56 ... 43 43 42]\n",
      " [42  1 39 ...  0  0 13]\n",
      " [13 50 50 ... 47 64 43]\n",
      " ...\n",
      " [57 43 12 ... 63  0 47]\n",
      " [47 57  1 ...  1 46 43]\n",
      " [43 56 43 ... 51 43  6]]\n"
     ]
    }
   ],
   "source": [
    "class DataLoader: \n",
    "    def __init__(self, fpath, batch_size, block_size) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.fpath = fpath\n",
    "        # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "        # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "        data = np.memmap(self.fpath, dtype=np.uint16, mode='r')\n",
    "        n_blocks = np.ceil((len(data)-1)/(block_size-1)).astype(int)\n",
    "        self.idx = np.arange(n_blocks*block_size).reshape(-1, block_size) - np.arange(n_blocks).reshape(-1, 1)\n",
    "        self.idx = self.idx % len(data)\n",
    "\n",
    "    def __iter__(self): \n",
    "        self.i = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self): \n",
    "        if self.i >= self.idx.shape[0]: \n",
    "            raise StopIteration()\n",
    "        else: \n",
    "            data = np.memmap(self.fpath, dtype=np.uint16, mode='r')\n",
    "            batch_idx = self.idx[self.i:self.i+batch_size]\n",
    "            return data[batch_idx]\n",
    "            self.i += self.batch_size\n",
    "    \n",
    "dl = DataLoader(os.path.join(data_dir, 'train.bin'), batch_size=8, block_size=32)\n",
    "for bla in itertools.islice(dl, 4): \n",
    "    print(bla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (4, 5)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RestartableBatchIterator:\n",
    "    def __init__(self, iterable, batch_size):\n",
    "        self.iterable = iterable\n",
    "        self.batch_size = batch_size\n",
    "    def __iter__(self):\n",
    "        self.iter = iter(itertools.batched(self.iterable, self.batch_size))\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        return next(self.iter)\n",
    "    \n",
    "list(RestartableBatchIterator([1,2,3,4,5], 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
